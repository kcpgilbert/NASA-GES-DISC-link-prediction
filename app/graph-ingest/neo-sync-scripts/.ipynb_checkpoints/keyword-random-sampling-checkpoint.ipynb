{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "78d795e1-013c-4af0-a76a-592be3020568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('PredictedKeywords.csv')\n",
    "df_missions = pd.read_csv('curation-missions-collection-names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "727f3e2e-6f6a-474a-a943-e1eccf8eb792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Collection</th>\n",
       "      <th>Title</th>\n",
       "      <th>Version</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Dataset Progress</th>\n",
       "      <th>Level</th>\n",
       "      <th>Granularity</th>\n",
       "      <th>Granule Size</th>\n",
       "      <th>Format</th>\n",
       "      <th>Creation Date</th>\n",
       "      <th>...</th>\n",
       "      <th>L34RS</th>\n",
       "      <th>GeoTiff</th>\n",
       "      <th>Giovanni</th>\n",
       "      <th>AIRS L1L2</th>\n",
       "      <th>WCS</th>\n",
       "      <th>WMS</th>\n",
       "      <th>THREDDS</th>\n",
       "      <th>OPeNDAP</th>\n",
       "      <th>GDS</th>\n",
       "      <th>Worldview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACOSMonthlyGriddedXCO2</td>\n",
       "      <td>Monthly Gridded Level 4 bias-corrected XCO2 an...</td>\n",
       "      <td>3</td>\n",
       "      <td>10.5067/6NHKPP06K0ZF</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>4</td>\n",
       "      <td>1 day</td>\n",
       "      <td>1.3 MB per file</td>\n",
       "      <td>netCDF</td>\n",
       "      <td>2022-02-11T00:00:00.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OPENDAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACOS_L2S</td>\n",
       "      <td>ACOS GOSAT/TANSO-FTS Level 2 Full Physics Stan...</td>\n",
       "      <td>7.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>2</td>\n",
       "      <td>sun-lit orbit (45 minutes)</td>\n",
       "      <td>3 MB per file</td>\n",
       "      <td>HDF5</td>\n",
       "      <td>2016-04-18T00:00:00.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OPENDAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACOS_L2S</td>\n",
       "      <td>ACOS GOSAT/TANSO-FTS Level 2 Full Physics Stan...</td>\n",
       "      <td>9r</td>\n",
       "      <td>10.5067/OSGTIL9OV0PN</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>2</td>\n",
       "      <td>sun-lit orbit (45 minutes)</td>\n",
       "      <td>3 MB per file</td>\n",
       "      <td>HDF5</td>\n",
       "      <td>2019-10-04T00:00:00.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OPENDAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACOS_L2_Lite_FP</td>\n",
       "      <td>ACOS GOSAT/TANSO-FTS Level 2 bias-corrected XC...</td>\n",
       "      <td>7.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>2</td>\n",
       "      <td>1 day</td>\n",
       "      <td>50 MB per file</td>\n",
       "      <td>netCDF</td>\n",
       "      <td>2016-10-28T00:00:00.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OPENDAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACOS_L2_Lite_FP</td>\n",
       "      <td>ACOS GOSAT/TANSO-FTS Level 2 bias-corrected XC...</td>\n",
       "      <td>9r</td>\n",
       "      <td>10.5067/VWSABTO7ZII4</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>2</td>\n",
       "      <td>1 day</td>\n",
       "      <td>50 MB per file</td>\n",
       "      <td>netCDF</td>\n",
       "      <td>2019-12-30T00:00:00.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OPENDAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Collection                                              Title  \\\n",
       "0  ACOSMonthlyGriddedXCO2  Monthly Gridded Level 4 bias-corrected XCO2 an...   \n",
       "1                ACOS_L2S  ACOS GOSAT/TANSO-FTS Level 2 Full Physics Stan...   \n",
       "2                ACOS_L2S  ACOS GOSAT/TANSO-FTS Level 2 Full Physics Stan...   \n",
       "3         ACOS_L2_Lite_FP  ACOS GOSAT/TANSO-FTS Level 2 bias-corrected XC...   \n",
       "4         ACOS_L2_Lite_FP  ACOS GOSAT/TANSO-FTS Level 2 bias-corrected XC...   \n",
       "\n",
       "  Version                   DOI Dataset Progress Level  \\\n",
       "0       3  10.5067/6NHKPP06K0ZF         COMPLETE     4   \n",
       "1     7.3                   NaN         COMPLETE     2   \n",
       "2      9r  10.5067/OSGTIL9OV0PN         COMPLETE     2   \n",
       "3     7.3                   NaN           ACTIVE     2   \n",
       "4      9r  10.5067/VWSABTO7ZII4           ACTIVE     2   \n",
       "\n",
       "                  Granularity     Granule Size  Format  \\\n",
       "0                       1 day  1.3 MB per file  netCDF   \n",
       "1  sun-lit orbit (45 minutes)    3 MB per file    HDF5   \n",
       "2  sun-lit orbit (45 minutes)    3 MB per file    HDF5   \n",
       "3                       1 day   50 MB per file  netCDF   \n",
       "4                       1 day   50 MB per file  netCDF   \n",
       "\n",
       "              Creation Date  ... L34RS GeoTiff Giovanni AIRS L1L2  WCS  WMS  \\\n",
       "0  2022-02-11T00:00:00.000Z  ...   NaN     NaN      NaN       NaN  NaN  NaN   \n",
       "1  2016-04-18T00:00:00.000Z  ...   NaN     NaN      NaN       NaN  NaN  NaN   \n",
       "2  2019-10-04T00:00:00.000Z  ...   NaN     NaN      NaN       NaN  NaN  NaN   \n",
       "3  2016-10-28T00:00:00.000Z  ...   NaN     NaN      NaN       NaN  NaN  NaN   \n",
       "4  2019-12-30T00:00:00.000Z  ...   NaN     NaN      NaN       NaN  NaN  NaN   \n",
       "\n",
       "   THREDDS  OPeNDAP  GDS Worldview  \n",
       "0      NaN  OPENDAP  NaN       NaN  \n",
       "1      NaN  OPENDAP  NaN       NaN  \n",
       "2      NaN  OPENDAP  NaN       NaN  \n",
       "3      NaN  OPENDAP  NaN       NaN  \n",
       "4      NaN  OPENDAP  NaN       NaN  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_missions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd4d5d95-d532-48d4-8bae-4b9f78506b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(532, 6)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "151cd535-ae9e-4dbd-820f-1a2ad92f0442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n"
     ]
    }
   ],
   "source": [
    "unique_datasets_count = df['DATASET_SHORT'].nunique()\n",
    "print(unique_datasets_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "851e278b-cd02-4fe9-bf2f-2cf66ed269cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_LONG\n",
      "Goddard Satellite-Based Surface Turbulent Fluxes, 1x1 deg Daily Grid, Set1 V2c (GSSTF) at GES DISC                                                     13\n",
      "Sounder SIPS: JPSS-1 ATMS Level 2 RAMSES2 Support V3 (SNDRJ1ML2RMSSUP) at GES DISC                                                                     12\n",
      "AIRS/Aqua L2 Near Real Time (NRT) Standard Physical Retrieval (AIRS-only) V006 (AIRS2RET_NRT) at GES DISC                                              10\n",
      "Aqua/AIRS L2 Support Retrieval (AIRS+AMSU+HSB) V7.0 at GES DISC                                                                                        10\n",
      "Goddard Satellite-Based Surface Turbulent Fluxes, 0.25 x 0.25 deg, Daily Grid F08 V3 (GSSTF_F08) at GES DISC                                           10\n",
      "                                                                                                                                                       ..\n",
      "Aqua/AIRS L2 Standard Physical Retrieval (AIRS-only) V7.0 at GES DISC                                                                                   1\n",
      "MERRA-2 inst3_3d_aer_Nv: 3d,3-Hourly,Instantaneous,Model-Level,Assimilation,Aerosol Mixing Ratio 0.625 x 0.5 degree V5.12.4 (M2I3NVAER) at GES DISC     1\n",
      "AIRS/Aqua L2G Precipitation Estimate (AIRS-only) V7.0 at GES DISC                                                                                       1\n",
      "Sounder SIPS: AQUA AIRS IR-only Level 3 CLIMCAPS: Comprehensive Quality Control Gridded Monthly V2 (SNDRAQIL3CMCCP) at GES DISC                         1\n",
      "MLS/Aura Level 3 Daily Binned Ozone (O3) Mixing Ratio on Zonal and Similar Grids V005 (ML3DZO3) at GES DISC                                             1\n",
      "Name: count, Length: 184, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get the counts of each unique name in the 'DATASET_SHORT' column\n",
    "name_counts = df['DATASET_LONG'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(name_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3472e5-7828-4069-a73a-e7a8fb406a8f",
   "metadata": {},
   "source": [
    "## AIRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eefd6904-db01-4af3-a459-705768ef7f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    DATASET_SHORT        KEYWORD  probability  \\\n",
      "135      AIRH3SPM  surface winds          1.0   \n",
      "\n",
      "                                          DATASET_LONG  DATASET_NODE_ID  \\\n",
      "135  AIRS/Aqua L3 Monthly Support Product (AIRS+AMS...              912   \n",
      "\n",
      "     KEYWORD_NODE_ID  \n",
      "135             1749  \n",
      "\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "AIRS_identifier = 'AIR'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "AIRS_df = df[df['DATASET_SHORT'].str.contains(AIRS_identifier, case=False, na=False)]\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "#print(AIRS_df)\n",
    "\n",
    "# Group by the 'DATASET_SHORT' column\n",
    "AIRS_grouped = AIRS_df.groupby('DATASET_SHORT')\n",
    "\n",
    "# Randomly sample one group\n",
    "sampled_AIRS = AIRS_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "sampled_AIRS_df = AIRS_grouped.get_group(sampled_AIRS)\n",
    "\n",
    "# Print the sampled group DataFrame\n",
    "print(sampled_AIRS_df)\n",
    "print('\\n')\n",
    "print(len(sampled_AIRS_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f30e75-c6cf-4f07-8b39-1ae9d810ac5f",
   "metadata": {},
   "source": [
    "## ATDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5b09d364-dcad-4b75-b092-038453bc0d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    DATASET_SHORT KEYWORD  probability  \\\n",
      "140       MAM05S0  clouds          1.0   \n",
      "\n",
      "                                          DATASET_LONG  DATASET_NODE_ID  \\\n",
      "140  MODIS/Aqua Total Precip Water Vapor 1km and 5k...             1136   \n",
      "\n",
      "     KEYWORD_NODE_ID  \n",
      "140             1674  \n",
      "\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# The specific value to search for\n",
    "ATDD_identifier = 'ATDD'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'Column1' equals the specific value\n",
    "df_missions_ATDD = df_missions[df_missions['Mission/Project'] == ATDD_identifier]\n",
    "\n",
    "#print(df_missions_ATDD)\n",
    "\n",
    "# Get the list of values from the 'Collection' column of df_missions_ATDD\n",
    "ATDD_collections = df_missions_ATDD['Collection'].unique()\n",
    "\n",
    "# Filter the df DataFrame to include only rows where 'DATASET_SHORT' matches any value in collection_values\n",
    "ATDD_df = df[df['DATASET_SHORT'].isin(ATDD_collections)]\n",
    "\n",
    "if len(ATDD_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    ATDD_grouped = ATDD_df.groupby('DATASET_SHORT')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_ATDD = ATDD_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_ATDD_df = ATDD_grouped.get_group(sampled_ATDD)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_ATDD_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_ATDD_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba2e1af-22b6-4fe4-a273-32daad8a06ed",
   "metadata": {},
   "source": [
    "## ATMOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "13edf708-58b6-4570-8aa2-09d5cab53d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reccomendations\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "ATMOS_identifier = 'ATMOS'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "ATMOS_df = df[df['DATASET_SHORT'].str.contains(ATMOS_identifier, case=False, na=False)]\n",
    "\n",
    "if len(ATMOS_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    ATMOS_grouped = ATMOS_df.groupby('DATASET_SHORT')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_ATMOS = ATMOS_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_ATMOS_df = ATMOS_grouped.get_group(sampled_ATMOS)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_ATMOS_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_ATMOS_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1b958d-05e2-4c87-8df0-3221fc823759",
   "metadata": {},
   "source": [
    "## CAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "efb21b78-a041-4d53-8391-85a079895da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reccomendations\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "CAR_identifier = 'CAR'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "CAR_df = df[df['DATASET_SHORT'].str.contains(CAR_identifier, case=False, na=False)]\n",
    "\n",
    "if len(CAR_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    CAR_grouped = CAR_df.groupby('DATASET_SHORT')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_CAR = CAR_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_CAR_df = CAR_grouped.get_group(sampled_CAR)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_CAR_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_CAR_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edd0b3a-ddcd-4932-86a5-e3efe2cc913c",
   "metadata": {},
   "source": [
    "## CMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fce1a294-7729-4e37-80ef-0209ba431094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reccomendations\n"
     ]
    }
   ],
   "source": [
    "# The specific value to search for\n",
    "CMS_identifier = 'CMS'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'Column1' equals the specific value\n",
    "df_missions_CMS = df_missions[df_missions['Mission/Project'] == CMS_identifier]\n",
    "\n",
    "#print(df_missions_ATDD)\n",
    "\n",
    "# Get the list of values from the 'Collection' column of df_missions_ATDD\n",
    "CMS_collections = df_missions_CMS['Collection'].unique()\n",
    "\n",
    "# Filter the df DataFrame to include only rows where 'DATASET_SHORT' matches any value in collection_values\n",
    "CMS_df = df[df['DATASET_SHORT'].isin(CMS_collections)]\n",
    "\n",
    "if len(CMS_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    CMS_grouped = CMS_df.groupby('DATASET_SHORT')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_CMS = CMS_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_CMS_df = CMS_grouped.get_group(sampled_CMS)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_CMS_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_CMS_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec61b99-b254-40c2-85ed-791936d20f51",
   "metadata": {},
   "source": [
    "## GPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f8610a13-11f6-451f-9e3f-1c31e02c0588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reccomendations\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "GPM_identifier = 'GPM'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "GPM_df = df[df['DATASET_SHORT'].str.contains(GPM_identifier, case=False, na=False)]\n",
    "\n",
    "if len(GPM_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    GPM_grouped = GPM_df.groupby('DATASET_SHORT')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_GPM = GPM_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_GPM_df = GPM_grouped.get_group(sampled_GPM)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_GPM_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_GPM_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738971b4-f0b9-486d-96e9-99357bcb716b",
   "metadata": {},
   "source": [
    "## HAQAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "11dfc1d4-63b5-4d21-bb3e-bd6fa1eadd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reccomendations\n"
     ]
    }
   ],
   "source": [
    "# The specific value to search for\n",
    "HAQAST_identifier = 'HAQAST'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'Column1' equals the specific value\n",
    "df_missions_HAQAST = df_missions[df_missions['Mission/Project'] == HAQAST_identifier]\n",
    "\n",
    "#print(df_missions_ATDD)\n",
    "\n",
    "# Get the list of values from the 'Collection' column of df_missions_ATDD\n",
    "HAQAST_collections = df_missions_HAQAST['Collection'].unique()\n",
    "\n",
    "# Filter the df DataFrame to include only rows where 'DATASET_SHORT' matches any value in collection_values\n",
    "HAQAST_df = df[df['DATASET_SHORT'].isin(HAQAST_collections)]\n",
    "\n",
    "if len(HAQAST_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    HAQAST_grouped = HAQAST_df.groupby('DATASET_SHORT')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_HAQAST = HAQAST_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_HAQAST_df = HAQAST_grouped.get_group(sampled_HAQAST)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_HAQAST_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_HAQAST_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b91c770-a3f3-454a-8167-9cbb94dc7f8c",
   "metadata": {},
   "source": [
    "## HIRDLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c00e27f1-60b0-4bfd-a164-f90a711873c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reccomendations\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "HIRDLS_identifier = 'HIRDLS'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "HIRDLS_df = df[df['DATASET_LONG'].str.contains(HIRDLS_identifier, case=False, na=False)]\n",
    "\n",
    "if len(HIRDLS_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    HIRDLS_grouped = HIRDLS_df.groupby('DATASET_LONG')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_HIRDLS = HIRDLS_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_HIRDLS_df = HIRDLS_grouped.get_group(sampled_HIRDLS)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_HIRDLS_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_HIRDLS_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e73e0b-15fc-492b-89a8-460d100e7a2f",
   "metadata": {},
   "source": [
    "## Hydrology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3d91e806-74b5-4456-9c98-7b9b0e994848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DATASET_SHORT                KEYWORD  probability  \\\n",
      "96   FLDAS_NOAH01_C_GL_MC       oxygen compounds         1.00   \n",
      "384  FLDAS_NOAH01_C_GL_MC  upper air temperature         0.99   \n",
      "385  FLDAS_NOAH01_C_GL_MC      atmospheric ozone         0.99   \n",
      "\n",
      "                                          DATASET_LONG  DATASET_NODE_ID  \\\n",
      "96   FLDAS Noah Land Surface Model L4 Global Monthl...              388   \n",
      "384  FLDAS Noah Land Surface Model L4 Global Monthl...              388   \n",
      "385  FLDAS Noah Land Surface Model L4 Global Monthl...              388   \n",
      "\n",
      "     KEYWORD_NODE_ID  \n",
      "96              1633  \n",
      "384             1670  \n",
      "385             1634  \n",
      "\n",
      "\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "Hydrology_identifiers = ['GRACE', 'NEWS', 'GLDAS', 'NLDAS', 'FLDAS', 'NCA-LDAS', 'WLDAS', 'LPRM']\n",
    "\n",
    "# Create a regular expression pattern that matches any of the identifiers\n",
    "pattern = '|'.join(Hydrology_identifiers)\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_LONG' contains any of the specific words\n",
    "Hydrology_df = df[df['DATASET_SHORT'].str.contains(pattern, case=False, na=False)]\n",
    "\n",
    "if len(Hydrology_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    Hydrology_grouped = Hydrology_df.groupby('DATASET_SHORT')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_Hydrology = Hydrology_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_Hydrology_df = Hydrology_grouped.get_group(sampled_Hydrology)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_Hydrology_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_Hydrology_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ae1384-1cf6-432a-8793-c8bc7f4520ce",
   "metadata": {},
   "source": [
    "## Legacy/GPCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fe461636-f805-48bf-8ca1-a925bb12764d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reccomendations\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "GPCP_identifier = 'GPCP'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "GPCP_df = df[df['DATASET_SHORT'].str.contains(GPCP_identifier, case=False, na=False)]\n",
    "\n",
    "if len(GPCP_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    GPCP_grouped = GPCP_df.groupby('DATASET_SHORT')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_GPCP = GPCP_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_GPCP_df = GPCP_grouped.get_group(sampled_GPCP)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_GPCP_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_GPCP_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35d165a-d01b-4f9c-8b2e-be85a0b4e154",
   "metadata": {},
   "source": [
    "## Legacy/Nimbus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eebf024c-4f16-44a6-b52a-4bd2f52c6e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    DATASET_SHORT   KEYWORD  probability  \\\n",
      "483    BUVN4L3ZMT  aerosols         0.99   \n",
      "\n",
      "                                          DATASET_LONG  DATASET_NODE_ID  \\\n",
      "483  BUV/Nimbus-4 Level 3 Ozone Zonal Means V005 (B...             1525   \n",
      "\n",
      "     KEYWORD_NODE_ID  \n",
      "483             1607  \n",
      "\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "Nimbus_identifier = 'Nimbus'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "Nimbus_df = df[df['DATASET_LONG'].str.contains(Nimbus_identifier, case=False, na=False)]\n",
    "\n",
    "if len(Nimbus_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    Nimbus_grouped = Nimbus_df.groupby('DATASET_LONG')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_Nimbus = Nimbus_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_Nimbus_df = Nimbus_grouped.get_group(sampled_Nimbus)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_Nimbus_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_Nimbus_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9659afc-17e7-4b10-b483-57e93b0bbaa7",
   "metadata": {},
   "source": [
    "## MEaSUREs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0dd7bc5c-e7bf-4847-8deb-295deef96c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    DATASET_SHORT           KEYWORD  probability  \\\n",
      "491  MUSUMLSL2DGG  cloud properties         0.99   \n",
      "492  MUSUMLSL2DGG     precipitation         0.99   \n",
      "\n",
      "                                          DATASET_LONG  DATASET_NODE_ID  \\\n",
      "491  MUSTARD-UMLS Level 2 Standard Products V1 (MUS...             1221   \n",
      "492  MUSTARD-UMLS Level 2 Standard Products V1 (MUS...             1221   \n",
      "\n",
      "     KEYWORD_NODE_ID  \n",
      "491             1686  \n",
      "492             1585  \n",
      "\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# The specific value to search for\n",
    "Measures_identifier = 'MEaSUREs'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'Column1' equals the specific value\n",
    "df_missions_Measures = df_missions[df_missions['Mission/Project'] == Measures_identifier]\n",
    "\n",
    "#print(df_missions_ATDD)\n",
    "\n",
    "# Get the list of values from the 'Collection' column of df_missions_ATDD\n",
    "Measures_collections = df_missions_Measures['Collection'].unique()\n",
    "\n",
    "# Filter the df DataFrame to include only rows where 'DATASET_SHORT' matches any value in collection_values\n",
    "Measures_df = df[df['DATASET_SHORT'].isin(Measures_collections)]\n",
    "\n",
    "if len(Measures_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    Measures_grouped = Measures_df.groupby('DATASET_SHORT')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_Measures = Measures_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_Measures_df = Measures_grouped.get_group(sampled_Measures)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_Measures_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_Measures_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a20c7b-db79-4a5b-aedf-ee376660ced9",
   "metadata": {},
   "source": [
    "## MERRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7819f647-0178-4dd0-bd15-9aecd1657b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    DATASET_SHORT                KEYWORD  probability  \\\n",
      "404     M2IUNXLFO  upper air temperature         0.99   \n",
      "\n",
      "                                          DATASET_LONG  DATASET_NODE_ID  \\\n",
      "404  MERRA-2 instU_2d_lfo_Nx: 2d,diurnal,Instantane...              679   \n",
      "\n",
      "     KEYWORD_NODE_ID  \n",
      "404             1670  \n",
      "\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "merra_identifier = 'MERRA'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "merra_df = df[df['DATASET_LONG'].str.contains(merra_identifier, case=False, na=False)]\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "#print(merra_df)\n",
    "\n",
    "# Group by the 'DATASET_SHORT' column\n",
    "merra_grouped = merra_df.groupby('DATASET_LONG')\n",
    "\n",
    "# Randomly sample one group\n",
    "sampled_merra = merra_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "sampled_merra_df = merra_grouped.get_group(sampled_merra)\n",
    "\n",
    "# Print the sampled group DataFrame\n",
    "print(sampled_merra_df)\n",
    "print('\\n')\n",
    "print(len(sampled_merra_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd7700a-e521-4b1e-9838-0838d6c87eca",
   "metadata": {},
   "source": [
    "## MLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "59efcead-4207-4ed2-a8bd-e7fdb0242771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    DATASET_SHORT   KEYWORD  probability  \\\n",
      "213         ML2O3  aerosols          1.0   \n",
      "\n",
      "                                          DATASET_LONG  DATASET_NODE_ID  \\\n",
      "213  MLS/Aura Level 2 Ozone (O3) Mixing Ratio V005 ...              643   \n",
      "\n",
      "     KEYWORD_NODE_ID  \n",
      "213             1607  \n",
      "\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "MLS_identifier = 'MLS'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "MLS_df = df[df['DATASET_LONG'].str.contains(MLS_identifier, case=False, na=False)]\n",
    "\n",
    "if len(MLS_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    MLS_grouped = MLS_df.groupby('DATASET_LONG')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_MLS = MLS_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_MLS_df = MLS_grouped.get_group(sampled_MLS)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_MLS_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_MLS_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba68535-c83c-438b-a77d-51f2e50279dd",
   "metadata": {},
   "source": [
    "## NEESPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2fd8fe28-90e0-41ce-a686-f7ea0448c058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reccomendations\n"
     ]
    }
   ],
   "source": [
    "# The specific value to search for\n",
    "NEESPI_identifier = 'NEESPI NASA'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'Column1' equals the specific value\n",
    "df_missions_NEESPI = df_missions[df_missions['Mission/Project'] == NEESPI_identifier]\n",
    "\n",
    "#print(df_missions_ATDD)\n",
    "\n",
    "# Get the list of values from the 'Collection' column of df_missions_ATDD\n",
    "NEESPI_collections = df_missions_NEESPI['Collection'].unique()\n",
    "\n",
    "# Filter the df DataFrame to include only rows where 'DATASET_SHORT' matches any value in collection_values\n",
    "NEESPI_df = df[df['DATASET_SHORT'].isin(NEESPI_collections)]\n",
    "\n",
    "if len(NEESPI_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    NEESPI_grouped = NEESPI_df.groupby('DATASET_SHORT')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_NEESPI = NEESPI_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_NEESPI_df = NEESPI_grouped.get_group(sampled_NEESPI)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_NEESPI_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_NEESPI_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad63c93-4fbb-41f8-8e73-639bb73887a3",
   "metadata": {},
   "source": [
    "## NOBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8afc79c6-47be-427e-85ae-d1d22db18764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reccomendations\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "NOBM_identifier = 'NOBM'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "NOBM_df = df[df['DATASET_SHORT'].str.contains(NOBM_identifier, case=False, na=False)]\n",
    "\n",
    "if len(NOBM_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    NOBM_grouped = NOBM_df.groupby('DATASET_SHORT')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_NOBM = NOBM_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_NOBM_df = NOBM_grouped.get_group(sampled_NOBM)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_NOBM_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_NOBM_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81365567-1e0a-4412-bc95-c9ec6e45850b",
   "metadata": {},
   "source": [
    "## OCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "97b2a640-5155-4127-b2c4-6cbbc245bc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reccomendations\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "OCO_identifier = 'OCO'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "OCO_df = df[df['DATASET_SHORT'].str.contains(OCO_identifier, case=False, na=False)]\n",
    "\n",
    "if len(OCO_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    OCO_grouped = OCO_df.groupby('DATASET_SHORT')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_OCO = OCO_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_OCO_df = OCO_grouped.get_group(sampled_OCO)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_OCO_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_OCO_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64602260-93a4-463c-94df-8145e4e0b1a3",
   "metadata": {},
   "source": [
    "## OMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "00cb6262-685e-4519-890b-7510d1c23ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    DATASET_SHORT                KEYWORD  probability  \\\n",
      "412       OMCLDRR  upper air temperature         0.99   \n",
      "\n",
      "                                          DATASET_LONG  DATASET_NODE_ID  \\\n",
      "412  OMI/Aura Effective Cloud Pressure and Fraction...             1532   \n",
      "\n",
      "     KEYWORD_NODE_ID  \n",
      "412             1670  \n",
      "\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "OMI_identifier = 'OMI'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "OMI_df = df[df['DATASET_LONG'].str.contains(OMI_identifier, case=True, na=False)]\n",
    "\n",
    "if len(OMI_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    OMI_grouped = OMI_df.groupby('DATASET_LONG')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_OMI = OMI_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_OMI_df = OMI_grouped.get_group(sampled_OMI)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_OMI_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_OMI_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f104d90-163b-40e4-b003-521f04da3bfd",
   "metadata": {},
   "source": [
    "## OMPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "02ea5323-9550-45fa-9816-0d47676dfaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reccomendations\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "OMPS_identifier = 'OMPS'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "OMPS_df = df[df['DATASET_SHORT'].str.contains(OMPS_identifier, case=False, na=False)]\n",
    "\n",
    "if len(OMPS_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    OMPS_grouped = OMPS_df.groupby('DATASET_SHORT')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_OMPS = OMPS_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_OMPS_df = OMPS_grouped.get_group(sampled_OMPS)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_OMPS_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_OMPS_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fef50a-63a6-4b89-b2d0-96a4afb75eca",
   "metadata": {},
   "source": [
    "## POES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "88adb0b0-b848-4ae9-b25a-623e0ca5b0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reccomendations\n"
     ]
    }
   ],
   "source": [
    "# The specific value to search for\n",
    "POES_identifier = 'POES'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'Column1' equals the specific value\n",
    "df_missions_POES = df_missions[df_missions['Mission/Project'] == POES_identifier]\n",
    "\n",
    "#print(df_missions_ATDD)\n",
    "\n",
    "# Get the list of values from the 'Collection' column of df_missions_ATDD\n",
    "POES_collections = df_missions_POES['Collection'].unique()\n",
    "\n",
    "# Filter the df DataFrame to include only rows where 'DATASET_SHORT' matches any value in collection_values\n",
    "POES_df = df[df['DATASET_SHORT'].isin(POES_collections)]\n",
    "\n",
    "if len(POES_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    POES_grouped = POES_df.groupby('DATASET_SHORT')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_POES = POES_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_POES_df = POES_grouped.get_group(sampled_POES)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_POES_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_POES_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b0caa-8f41-4843-a581-4b193c6c3906",
   "metadata": {},
   "source": [
    "## SBUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7f1b83b6-88f9-4b92-8280-5516f788912d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    DATASET_SHORT   KEYWORD  probability  \\\n",
      "434    SBUV2N14L2  aerosols         0.99   \n",
      "\n",
      "                                          DATASET_LONG  DATASET_NODE_ID  \\\n",
      "434  SBUV2/NOAA-14 Ozone (O3) Nadir Profile and Tot...             1240   \n",
      "\n",
      "     KEYWORD_NODE_ID  \n",
      "434             1607  \n",
      "\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "SBUV_identifier = 'SBUV'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "SBUV_df = df[df['DATASET_SHORT'].str.contains(SBUV_identifier, case=False, na=False)]\n",
    "\n",
    "if len(SBUV_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    SBUV_grouped = SBUV_df.groupby('DATASET_SHORT')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_SBUV = SBUV_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_SBUV_df = SBUV_grouped.get_group(sampled_SBUV)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_SBUV_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_SBUV_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25838c4-c7d8-4007-b0dc-8f9b6b95a2c6",
   "metadata": {},
   "source": [
    "## Sentinel-5P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7c1fdbd7-817e-4336-8847-d50b630f56e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      DATASET_SHORT                 KEYWORD  probability  \\\n",
      "378  S5P_L2__CLOUD_                humidity         0.99   \n",
      "379  S5P_L2__CLOUD_  water vapor indicators         0.99   \n",
      "\n",
      "                                          DATASET_LONG  DATASET_NODE_ID  \\\n",
      "378  Sentinel-5P TROPOMI Cloud 1-Orbit L2 7km x 3.5...             1534   \n",
      "379  Sentinel-5P TROPOMI Cloud 1-Orbit L2 7km x 3.5...             1534   \n",
      "\n",
      "     KEYWORD_NODE_ID  \n",
      "378             1672  \n",
      "379             1629  \n",
      "\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "S5P_identifier = 'S5P'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "S5P_df = df[df['DATASET_SHORT'].str.contains(S5P_identifier, case=False, na=False)]\n",
    "\n",
    "if len(S5P_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    S5P_grouped = S5P_df.groupby('DATASET_SHORT')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_S5P = S5P_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_S5P_df = S5P_grouped.get_group(sampled_S5P)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_S5P_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_S5P_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be436d2-b6c3-4515-83fd-81f825963037",
   "metadata": {},
   "source": [
    "## SORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d9297899-91e2-4036-8571-7d2073a41e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reccomendations\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "SOR_identifier = 'SOR'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "SOR_df = df[df['DATASET_SHORT'].str.contains(SOR_identifier, case=False, na=False)]\n",
    "\n",
    "if len(SOR_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    SOR_grouped = SOR_df.groupby('DATASET_SHORT')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_SOR = SOR_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_SOR_df = SOR_grouped.get_group(sampled_SOR)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_SOR_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_SOR_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a27f0c-adb0-4e81-9b83-2fd718b22530",
   "metadata": {},
   "source": [
    "## Sounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "79d9529d-aff9-4b67-92dc-31ef5d964175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     DATASET_SHORT                   KEYWORD  probability  \\\n",
      "128  SNDRSNML3MRMS          cloud properties          1.0   \n",
      "129  SNDRSNML3MRMS  total precipitable water          1.0   \n",
      "130  SNDRSNML3MRMS                    clouds          1.0   \n",
      "131  SNDRSNML3MRMS               water vapor          1.0   \n",
      "\n",
      "                                          DATASET_LONG  DATASET_NODE_ID  \\\n",
      "128  Sounder SIPS: Suomi NPP ATMS Level 3 RAMSES2 S...             1155   \n",
      "129  Sounder SIPS: Suomi NPP ATMS Level 3 RAMSES2 S...             1155   \n",
      "130  Sounder SIPS: Suomi NPP ATMS Level 3 RAMSES2 S...             1155   \n",
      "131  Sounder SIPS: Suomi NPP ATMS Level 3 RAMSES2 S...             1155   \n",
      "\n",
      "     KEYWORD_NODE_ID  \n",
      "128             1686  \n",
      "129             1685  \n",
      "130             1674  \n",
      "131             1630  \n",
      "\n",
      "\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "sounder_identifier = 'SNDR'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "sounder_df = df[df['DATASET_SHORT'].str.contains(sounder_identifier, case=False, na=False)]\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "#print(sounder_df)\n",
    "\n",
    "# Group by the 'DATASET_SHORT' column\n",
    "sounder_grouped = sounder_df.groupby('DATASET_SHORT')\n",
    "\n",
    "# Randomly sample one group\n",
    "sampled_sounder = sounder_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "sampled_sounder_df = sounder_grouped.get_group(sampled_sounder)\n",
    "\n",
    "# Print the sampled group DataFrame\n",
    "print(sampled_sounder_df)\n",
    "print('\\n')\n",
    "print(len(sampled_sounder_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751cda86-b1e4-40d0-9b7e-9eaec1d8ed51",
   "metadata": {},
   "source": [
    "## TCTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b237f11a-75b7-4841-9340-e42082784398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reccomendations\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "TCTE_identifier = 'TCTE'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "TCTE_df = df[df['DATASET_SHORT'].str.contains(TCTE_identifier, case=False, na=False)]\n",
    "\n",
    "if len(TCTE_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    TCTE_grouped = TCTE_df.groupby('DATASET_SHORT')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_TCTE = TCTE_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_TCTE_df = TCTE_grouped.get_group(sampled_TCTE)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_TCTE_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_TCTE_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b6a7ee-83ed-4bf8-a23b-18cba43ab71c",
   "metadata": {},
   "source": [
    "## TMPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fddb33ea-c430-4705-90e8-c5698494d541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reccomendations\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "TMPA_identifier = 'TMPA'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "TMPA_df = df[df['DATASET_LONG'].str.contains(TMPA_identifier, case=False, na=False)]\n",
    "\n",
    "if len(TMPA_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    TMPA_grouped = TMPA_df.groupby('DATASET_LONG')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_TMPA = TMPA_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_TMPA_df = TMPA_grouped.get_group(sampled_TMPA)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_TMPA_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_TMPA_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58bb7a4-e7d2-4ada-9c6f-00f02021a745",
   "metadata": {},
   "source": [
    "## TOMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bed04c95-eb8c-4227-8c9a-1e6f21c8f655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reccomendations\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "TOMS_identifier = 'TOMS'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "TOMS_df = df[df['DATASET_LONG'].str.contains(TOMS_identifier, case=False, na=False)]\n",
    "\n",
    "if len(TOMS_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    TOMS_grouped = TOMS_df.groupby('DATASET_LONG')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_TOMS = TOMS_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_TOMS_df = TOMS_grouped.get_group(sampled_TOMS)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_TOMS_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_TOMS_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5001ba5d-de74-4e20-9094-7bc33bfaba04",
   "metadata": {},
   "source": [
    "## TOVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cd3fff5d-b381-4d76-9133-8878da83fca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    DATASET_SHORT                   KEYWORD  probability  \\\n",
      "120      TOVSBMND  total precipitable water          1.0   \n",
      "121      TOVSBMND           air temperature          1.0   \n",
      "\n",
      "                                          DATASET_LONG  DATASET_NODE_ID  \\\n",
      "120  TOVS LMD MONTHLY GRIDS from NOAA-12 V01 (TOVSB...             1174   \n",
      "121  TOVS LMD MONTHLY GRIDS from NOAA-12 V01 (TOVSB...             1174   \n",
      "\n",
      "     KEYWORD_NODE_ID  \n",
      "120             1685  \n",
      "121             1680  \n",
      "\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "TOVS_identifier = 'TOVS'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "TOVS_df = df[df['DATASET_SHORT'].str.contains(TOVS_identifier, case=False, na=False)]\n",
    "\n",
    "if len(TOVS_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    TOVS_grouped = TOVS_df.groupby('DATASET_SHORT')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_TOVS = TOVS_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_TOVS_df = TOVS_grouped.get_group(sampled_TOVS)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_TOVS_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_TOVS_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba501e31-8bb3-4b55-b760-d8a7213eb7b7",
   "metadata": {},
   "source": [
    "## TRMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2c1a7f97-b3be-4e7e-91f7-4b549b294a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    DATASET_SHORT                KEYWORD  probability  \\\n",
      "185     TRMM_2B31  upper air temperature          1.0   \n",
      "\n",
      "                                          DATASET_LONG  DATASET_NODE_ID  \\\n",
      "185  TRMM Combined Precipitation Radar and Microwav...              282   \n",
      "\n",
      "     KEYWORD_NODE_ID  \n",
      "185             1670  \n",
      "\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "TRMM_identifier = 'TRMM'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "TRMM_df = df[df['DATASET_LONG'].str.contains(TRMM_identifier, case=False, na=False)]\n",
    "\n",
    "if len(TRMM_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    TRMM_grouped = TRMM_df.groupby('DATASET_LONG')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_TRMM = TRMM_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_TRMM_df = TRMM_grouped.get_group(sampled_TRMM)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_TRMM_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_TRMM_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e77094-7f20-47bf-ab8a-fcb5a432f5d5",
   "metadata": {},
   "source": [
    "## TROPESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "50566e46-5c6b-4682-b151-999dae77a8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reccomendations\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "TROPESS_identifier = 'TROPESS'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "TROPESS_df = df[df['DATASET_LONG'].str.contains(TROPESS_identifier, case=False, na=False)]\n",
    "\n",
    "if len(TROPESS_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    TROPESS_grouped = TROPESS_df.groupby('DATASET_LONG')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_TROPESS = TROPESS_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_TROPESS_df = TROPESS_grouped.get_group(sampled_TROPESS)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_TROPESS_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_TROPESS_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f1b561-dfeb-42e2-b60e-b9935d6375e5",
   "metadata": {},
   "source": [
    "## TROPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3dc81d64-38d6-4040-8499-522aa44bf925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reccomendations\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "TROPICS_identifier = 'TROPICS'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "TROPICS_df = df[df['DATASET_LONG'].str.contains(TROPICS_identifier, case=False, na=False)]\n",
    "\n",
    "if len(TROPICS_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    TROPICS_grouped = TROPICS_df.groupby('DATASET_LONG')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_TROPICS = TROPICS_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_TROPICS_df = TROPICS_grouped.get_group(sampled_TROPICS)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_TROPICS_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_TROPICS_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afa8acb-142d-4b21-bfa8-4ae1e7b7b88f",
   "metadata": {},
   "source": [
    "## TSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1edd7a51-4736-4004-a97a-d2d316fdd384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reccomendations\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "TSIS_identifier = 'TSIS'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "TSIS_df = df[df['DATASET_LONG'].str.contains(TSIS_identifier, case=False, na=False)]\n",
    "\n",
    "if len(TSIS_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    TSIS_grouped = TSIS_df.groupby('DATASET_LONG')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_TSIS = TSIS_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_TSIS_df = TSIS_grouped.get_group(sampled_TSIS)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_TSIS_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_TSIS_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b79e609-f115-4704-94a7-c502fb4b6f3f",
   "metadata": {},
   "source": [
    "## UARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3e3c5e48-9462-4833-a922-f270bdfa18c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    DATASET_SHORT           KEYWORD  probability  \\\n",
      "26       UARML3AT  cloud properties         1.00   \n",
      "27       UARML3AT            clouds         1.00   \n",
      "322      UARML3AT     precipitation         0.99   \n",
      "\n",
      "                                          DATASET_LONG  DATASET_NODE_ID  \\\n",
      "26   UARS Microwave Limb Sounder (MLS) Level 3AT V0...             1497   \n",
      "27   UARS Microwave Limb Sounder (MLS) Level 3AT V0...             1497   \n",
      "322  UARS Microwave Limb Sounder (MLS) Level 3AT V0...             1497   \n",
      "\n",
      "     KEYWORD_NODE_ID  \n",
      "26              1686  \n",
      "27              1674  \n",
      "322             1585  \n",
      "\n",
      "\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "UARS_identifier = 'UARS'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "UARS_df = df[df['DATASET_LONG'].str.contains(UARS_identifier, case=False, na=False)]\n",
    "\n",
    "if len(UARS_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    UARS_grouped = UARS_df.groupby('DATASET_LONG')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_UARS = UARS_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_UARS_df = UARS_grouped.get_group(sampled_UARS)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_UARS_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_UARS_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ddd5a1-e2e8-4733-a84d-b4b4b9a35612",
   "metadata": {},
   "source": [
    "## VISSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "45f6bc4c-74e9-4c7e-a56c-a25f635cac40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reccomendations\n"
     ]
    }
   ],
   "source": [
    "# Define the specific word to search for\n",
    "VISSR_identifier = 'VISSR'\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'DATASET_SHORT' contains the specific word\n",
    "VISSR_df = df[df['DATASET_LONG'].str.contains(VISSR_identifier, case=False, na=False)]\n",
    "\n",
    "if len(VISSR_df) > 0:\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    # print(ATMOS_df)\n",
    "\n",
    "    # Group by the 'DATASET_SHORT' column\n",
    "    VISSR_grouped = VISSR_df.groupby('DATASET_LONG')\n",
    "\n",
    "    # Randomly sample one group\n",
    "    sampled_VISSR = VISSR_grouped.apply(lambda x: x.name).sample(n=1).values[0]\n",
    "    sampled_VISSR_df = VISSR_grouped.get_group(sampled_VISSR)\n",
    "\n",
    "    # Print the sampled group DataFrame\n",
    "    print(sampled_VISSR_df)\n",
    "    print('\\n')\n",
    "    print(len(sampled_VISSR_df))\n",
    "\n",
    "else:\n",
    "    print('No reccomendations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46d1dd7-6d27-4ce9-ba2e-362722aa0d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
